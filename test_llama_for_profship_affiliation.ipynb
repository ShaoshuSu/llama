{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 81.75 MiB is free. Including non-PyTorch memory, this process has 22.64 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 3.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mLLAMA_Generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama-2-13b-chat/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/llms/llama/llama_generation.py:34\u001b[0m, in \u001b[0;36mLLAMA_Generation.__init__\u001b[0;34m(self, ckpt_dir, tokenizer_path, max_seq_len, max_batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m     ckpt_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-2-7b-chat/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m     tokenizer_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer.model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     32\u001b[0m     max_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n",
      "File \u001b[0;32m~/research/llms/llama/llama/generation.py:120\u001b[0m, in \u001b[0;36mLlama.build\u001b[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001b[0m\n\u001b[1;32m    118\u001b[0m model_args\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mn_words\n\u001b[1;32m    119\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_tensor_type(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mHalfTensor)\n\u001b[0;32m--> 120\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/research/llms/llama/llama/model.py:443\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(params\u001b[38;5;241m.\u001b[39mdim, eps\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mnorm_eps)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m ColumnParallelLinear(\n\u001b[1;32m    447\u001b[0m     params\u001b[38;5;241m.\u001b[39mdim, params\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[1;32m    448\u001b[0m )\n",
      "File \u001b[0;32m~/research/llms/llama/llama/model.py:375\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, layer_id, args)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mn_heads\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m FeedForward(\n\u001b[1;32m    377\u001b[0m     dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m    378\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m    379\u001b[0m     multiple_of\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmultiple_of,\n\u001b[1;32m    380\u001b[0m     ffn_dim_multiplier\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mffn_dim_multiplier,\n\u001b[1;32m    381\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id \u001b[38;5;241m=\u001b[39m layer_id\n",
      "File \u001b[0;32m~/research/llms/llama/llama/model.py:221\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwq \u001b[38;5;241m=\u001b[39m ColumnParallelLinear(\n\u001b[1;32m    208\u001b[0m     args\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m    209\u001b[0m     args\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x,\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk \u001b[38;5;241m=\u001b[39m ColumnParallelLinear(\n\u001b[1;32m    215\u001b[0m     args\u001b[38;5;241m.\u001b[39mdim,\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x,\n\u001b[1;32m    220\u001b[0m )\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv \u001b[38;5;241m=\u001b[39m \u001b[43mColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_kv_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    229\u001b[0m     args\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    230\u001b[0m     args\u001b[38;5;241m.\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    237\u001b[0m     (\n\u001b[1;32m    238\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m )\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.conda/envs/roboranking/lib/python3.9/site-packages/fairscale/nn/model_parallel/layers.py:262\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, gather_output, init_method, stride, keep_master_weight_for_test)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition \u001b[38;5;241m=\u001b[39m divide_and_check_no_remainder(out_features, world_size)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Parameters.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Note: torch.nn.functional.linear performs XA^T + b and as a result\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# we allocate the transpose.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 81.75 MiB is free. Including non-PyTorch memory, this process has 22.64 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 3.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from llama_generation import LLAMA_Generation\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12357'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "agent = LLAMA_Generation(\n",
    "        ckpt_dir='llama-2-13b-chat/',\n",
    "        tokenizer_path='tokenizer.model',\n",
    "        max_seq_len=512 * 2,\n",
    "        max_batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# judge professorship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.instructions_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      " Based on the information provided, I would respond with \"YES\" as Changhwan Kim is listed as a Professor at the Korea Institute of Science and Technology (KIST) in Seoul, South Korea. His titles and positions mentioned in the document include:\n",
      "* Member, IEEE\n",
      "* B.S. degree in mechanical engineering, M.S. degree in machine design engineering, and Ph.D. degree in mechanical engineering from various universities\n",
      "* Research Associate, Robotics and Automation Laboratory, University of Notre Dame, Notre Dame, IN, USA (2002-2004)\n",
      "* Professor, Korea Institute of Science and Technology (KIST), Seoul, South Korea (since 2004)\n",
      "Therefore, based on the information provided, Changhwan Kim holds the position of a Professor at KIST.\n",
      "3.963902473449707\n"
     ]
    }
   ],
   "source": [
    "# content_text =\\\n",
    "#     \"Changhwan Kim (Member, IEEE) received the B.S. degree in mechanical engineering and the M.S. degree in machine design engineering from Hanyang University, Seoul, South Korea, in 1993 and 1995, respectively, and the Ph.D. degree in mechanical engineering from The University of Iowa, Iowa City, IA, USA, in 2002. From 2002 to 2004, he was a Research Associate with the Robotics and Automation Laboratory, University of Notre Dame, Notre Dame, IN, USA. Since 2004, he has been working at the Korea Institute of Science and Technology (KIST), Seoul. His research interests include task and motion planning for robot manipulation and social robots.(Based on document published on 26 December 2022).\"\n",
    "\n",
    "\n",
    "content_text =\\\n",
    "\"\"\"\n",
    "Google Affiliation: The BioRobotics Institute, Scuola Superiore Sant'Anna \n",
    "IEEE Biography: \n",
    "Andrea Mariani, The BioRobotics Institute and Department of Excellence in Robotics and AI, Sant’Anna School of Advanced Studies, Pisa, 56025, Italy. \n",
    "E-mail: andrea.mariani@santannapisa.it.(Based on document published on 1 August 2022).\n",
    "\"\"\"\n",
    "\n",
    "# content_text = \"N/A\"\n",
    "# content_text = \"Yan Zhuang (Senior Member, IEEE) is not a professor\"\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "response, original_answer = agent.judge_professorship(content_text, verbose=False)\n",
    "\n",
    "\n",
    "# remove space in string\n",
    "# response = response.replace(\" \", \"\")\n",
    "print(response)\n",
    "print(original_answer)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'auther_database_v5_5_2_extended_no_score.json', 'r') as file:\n",
    "    author_database = json.load(file)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "min_papers = 10 \n",
    "count = 0\n",
    "\n",
    "for pid, author in author_database.items():\n",
    "    \n",
    "    # if 'robor_affiliation' not in author and [\"is_professor\"] == 'None':\n",
    "    if 'is_professor' in author:\n",
    "        if author['is_professor'] == 'None' and author['number_of_ieee_paper'] >10:\n",
    "        # if author['is_professor'] == 'None' and author['num_papers'] >10:\n",
    "        # if author['is_professor'] == 'None' and author['number_of_ieee_paper'] >10 and 'ieee_affiliation' not in author:\n",
    "            \n",
    "            print()\n",
    "            print(pid)\n",
    "            print(author['number_of_ieee_paper'])\n",
    "            print(author['ieee_affiliation'])\n",
    "            print(author['ieee_biography'])\n",
    "\n",
    "            if author['ieee_biography'] == 'N/A':\n",
    "                response = 'NO'\n",
    "                original_answer = 'N/A'\n",
    "            else:\n",
    "                response,original_answer = agent.judge_professorship(author['ieee_biography'], verbose=False)\n",
    "\n",
    "            start = time.time()\n",
    "            # remove space in string\n",
    "            # response = response.replace(\" \", \"\")\n",
    "            print(response)\n",
    "            print(original_answer)\n",
    "            print(time.time() - start)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count > 10:\n",
    "                break\n",
    "\n",
    "print(count)\n",
    "# print(f\" > {min_papers} papers \\t {count}\")\n",
    "\n",
    "\n",
    "# with open(f'auther_database_v5_5_1_extended_no_score.json', 'w') as file:\n",
    "#     json.dump(author_database, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv to string\n",
    "\n",
    "# with open('country-info3_3.csv', 'r') as file:\n",
    "#     author_database = json.load(file)\n",
    "\n",
    "import csv\n",
    "\n",
    "file_path = 'country-info3_3.csv'  # Replace this with your file path\n",
    "\n",
    "with open(file_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader, None)  # Skip the header row\n",
    "    first_column_values = [row[0] for row in reader if row]  # Check if row is not empty\n",
    "    csv_string = '\\n'.join(first_column_values)\n",
    "\n",
    "\n",
    "instruct_text = \\\n",
    "    \"Based on the given information, please provide which of the following institute is the people belong to. \" \\\n",
    "    \"If the information is not enough to determine the institute, please provide the answer as 'N/A'.\\n\" + \\\n",
    "csv_string\n",
    "\n",
    "agent.instructions_text = instruct_text\n",
    "\n",
    "content_text = \"'Centre for Advanced Robotics Technology Innovation (CARTIN)', 'School of Electrical and Electronic Engineering', 'Nanyang Technological University, Singapore'\"\n",
    "\n",
    "\n",
    "affiliation = agent.judge_affiliation(content_text, verbose=False)\n",
    "\n",
    "print(affiliation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete the agent and empty the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "import gc\n",
    "gc.collect()\n",
    "# clena the GPU memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboranking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
